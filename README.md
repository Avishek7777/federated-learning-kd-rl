# Federated Learning with Knowledge Distillation and RL-Based Client Selection

This project implements an advanced Federated Learning (FL) pipeline using:

- âœ… **Client Clustering** based on data, gradient, and performance similarity
- âœ… **RL-based Client Selection** using a Q-network to select the most valuable clients
- âœ… **Knowledge Distillation** from the best clusters to a global model
- âœ… Support for **Diverse Model Architectures** per client (CNN, ResNet, EfficientNet, etc.)
- âœ… Memory-optimized training and modular design for scalability

---

## ğŸ“ Project Structure

FL-KD-RL-Project/
â”‚
â”œâ”€â”€ config/ # Global configuration dictionary
â”œâ”€â”€ models/ # All model definitions and wrappers
â”œâ”€â”€ data/ # Dataset loading, splitting, transforms
â”œâ”€â”€ clients/ # Client training, clustering, RL agent
â”œâ”€â”€ distillation/ # Global fine-tuning, distillation, evaluation
â”œâ”€â”€ utils/ # Memory management, logging setup
â”œâ”€â”€ tests/ # Unit tests for core modules
â”‚
â”œâ”€â”€ main.py # Main federated training loop
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # You're here!

---

## ğŸš€ How to Run

### 1. Install dependencies (Python 3.8+)

```bash
pip install -r requirements.txt
```
